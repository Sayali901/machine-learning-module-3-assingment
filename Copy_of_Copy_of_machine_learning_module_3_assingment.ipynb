{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Regression Assingment Quedtions:\n",
        "\n",
        "1)What is Simple Linear Regression?\n",
        " - Simple linear regression is a statistical method that models the relationship between one independent variable (X) and one dependent variable (Y) using a straight line. It's used to predict or estimate the value of the dependent variable (outcome) based on the value of the independent variable.\n",
        "\n",
        "2)What are the key assumptions of Simple Linear Regression?\n",
        " - The key assumptions of simple linear regression are that the relationship between the independent and dependent variables is linear, errors have constant variance (homoscedasticity), errors are independent, and errors are normally distributed. Violations of these assumptions can lead to biased or unreliable results.\n",
        "\n",
        "\n",
        " 3)What does the coefficient m represent in the equation Y=m+c\n",
        "  - In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line. It indicates how steeply the line rises or falls as x increases. A larger 'm' value means a steeper slope, while a negative 'm' indicates a decreasing line.\n",
        "\n",
        "\n",
        "4)What does the intercept c represent in the equation Y=m+c\n",
        "- In the equation y = mx + c, the variable 'c' represents the y-intercept of the line. Specifically, it's the y-coordinate of the point where the line intersects the y-axis. The value of 'c' can be easily identified when the equation is in the form y = mx + c, and it corresponds to the constant term.  \n",
        "\n",
        "\n",
        "5)How do we calculate the slope m in Simple Linear Regression?\n",
        "- In simple linear regression, the slope m is calculated using the formula: m = r * (sy / sx), where r is the correlation coefficient, sy is the standard deviation of the dependent variable, and sx is the standard deviation of the independent variable.\n",
        "\n",
        "\n",
        "6)What is the purpose of the least squares method in Simple Linear Regression?\n",
        "- The least squares method in simple linear regression is used to find the line of best fit that minimizes the sum of the squared differences between the observed data points and the predicted values on the regression line. Essentially, it determines the line that comes closest to all the data points in a scatterplot, offering a model for predicting the dependent variable based on the independent variable.\n",
        "\n",
        "\n",
        "7)How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "- In simple linear regression, the coefficient of determination (R²) represents the proportion of variance in the dependent variable that is explained by the independent variable. It's a value between 0 and 1, with higher values indicating a stronger relationship and better model fit. For example, an R² of 0.70 means that 70% of the variability in the dependent variable can be attributed to the independent variable.\n",
        "\n",
        "\n",
        "8)What is Multiple Linear Regression?\n",
        "- Multiple linear regression is a statistical method that predicts the value of a dependent variable based on the values of two or more independent variables. It's an extension of simple linear regression, which examines the relationship between one dependent and one independent variable. Multiple linear regression helps understand how several independent variables influence a dependent variable, potentially leading to more accurate predictions than using a single predictor.\n",
        "\n",
        "\n",
        "\n",
        "9)What is the main difference between Simple and Multiple Linear Regression?\n",
        "- The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable. Simple linear regression uses one independent variable, while multiple linear regression uses two or more. This difference impacts the complexity of the model and the ability to account for multiple factors influencing the outcome.\n",
        "\n",
        "\n",
        "10)What are the key assumptions of Multiple Linear Regression?\n",
        "- The key assumptions of multiple linear regression include linearity, independence, homoscedasticity, and normality. These assumptions ensure reliable results in regression analysis.\n",
        "\n",
        "\n",
        "11)What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "- Heteroscedasticity in a multiple linear regression model refers to the non-constant variance of the error terms (residuals) across different values of the independent variables. This violates one of the core assumptions of ordinary least squares (OLS) regression, which assumes that the error terms have a constant variance (homoscedasticity). When heteroscedasticity is present, the OLS regression model can lead to biased and unreliable results.\n",
        "\n",
        "\n",
        "12)How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- To address multicollinearity in a multiple linear regression model, you can consider several strategies, including removing highly correlated variables, combining them, or using techniques like Principal Component Analysis (PCA) to reduce dimensionality and mitigate the issue according to Analytics Vidhya. Regularization techniques like Ridge regression can also be helpful.\n",
        "\n",
        "\n",
        "13)- What are some common techniques for transforming categorical variables for use in regression models\n",
        "- Several techniques transform categorical variables for use in regression models, including one-hot encoding, label encoding, and target encoding. One-hot encoding creates binary columns for each category, while label encoding assigns unique integers. Target encoding substitutes categories with the mean target value for that category.\n",
        "\n",
        "\n",
        "14)What is the role of interaction terms in Multiple Linear Regression\n",
        "- In Multiple Linear Regression, interaction terms are crucial for understanding how the effect of one independent variable on the dependent variable changes depending on the level of another independent variable. They help capture non-additive relationships between variables, meaning the combined influence of two or more variables on the outcome is not just the sum of their individual effects.\n",
        "\n",
        "\n",
        "15)How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "- The interpretation of the intercept in simple and multiple linear regression is the same: it represents the expected value of the dependent variable when all independent variables are zero. However, the context and relevance of the intercept can differ depending on the model and the variables being used.\n",
        "\n",
        "\n",
        "16)What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "- In regression analysis, the slope represents the change in the dependent variable for every one-unit change in the independent variable. It indicates the direction and magnitude of the relationship between the variables, impacting how accurately the model can predict the dependent variable's value based on the independent variable\n",
        "\n",
        "\n",
        "17)How does the intercept in a regression model provide context for the relationship between variables\n",
        "- In regression models, the intercept provides the expected value of the dependent variable when all independent variables are zero. This baseline value can be useful for understanding the relationship between variables, especially if the independent variables can logically take on a zero value, or when the intercept is used to predict the outcome when the independent variable is absent.\n",
        "\n",
        "\n",
        "18)What are the limitations of using R² as a sole measure of model performance?\n",
        "- One primary limitation of R-squared is its sensitivity to the number of predictors included in the model. As more variables are added, the model tends to capture more variability—even if some predictors are irrelevant—resulting in an inflated R-squared.\n",
        "\n",
        "\n",
        "19)- How would you interpret a large standard error for a regression coefficient?\n",
        "- A large standard error for a regression coefficient suggests that the estimated coefficient is less precise and less reliable as an estimate of the true population value. It indicates that the coefficient could vary significantly across different samples of the data.\n",
        "\n",
        "\n",
        "20)How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "- Heteroscedasticity in residual plots can be identified by observing a fan-shaped pattern where the spread of residuals (points on the plot) increases or decreases systematically as the fitted values (predicted values) change. This indicates that the variance of the errors is not constant across the range of predicted values. It's important to address heteroscedasticity because it violates the assumptions of many statistical tests, leading to inaccurate and unreliable inferences.\n",
        "\n",
        "21)What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "- A high R-squared and low adjusted R-squared in a multiple linear regression model suggest that the model might be overfitted, meaning it's capturing noise in the data rather than true relationships. While R-squared indicates a good fit with the added variables, the adjusted R-squared, which penalizes for including unnecessary predictors, shows that these added variables might not be providing significant improvements in the model's predictive power.\n",
        "\n",
        "\n",
        "22)Why is it important to scale variables in Multiple Linear Regression?\n",
        "- Scaling variables in multiple linear regression is beneficial for several reasons, including faster convergence of optimization algorithms like gradient descent, improved interpretability of coefficients, and preventing dominant features from overshadowing others.\n",
        "\n",
        "23)What is polynomial regression?\n",
        "- Polynomial regression is an extension of linear regression used to model non-linear relationships between variables. It fits a polynomial equation to the data, allowing for curves and bends in the predicted line. This is useful when a simple straight line isn't enough to capture the data's patterns.\n",
        "\n",
        "\n",
        "24)How does polynomial regression differ from linear regression?\n",
        "- Polynomial regression differs from linear regression in the type of relationship it models between variables. Linear regression fits a straight line to the data, while polynomial regression fits a curve, allowing for more complex, non-linear relationships. This difference arises from how the independent variables are used in the model; polynomial regression uses powers and products of the independent variables to create a polynomial equation.\n",
        "\n",
        "25) When is polynomial regression used?\n",
        "- Polynomial regression is used when the relationship between variables is non-linear and can be modeled by a polynomial function, such as a quadratic, cubic, or higher-degree curve. It's a form of regression analysis that allows for more complex relationships than linear regression.\n",
        "\n",
        "\n",
        "26)What is the general equation for polynomial regression?\n",
        "- The general equation for polynomial regression, where y is the dependent variable and x is the independent variable, is y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ϵ, where:\n",
        "y: is the predicted value of the dependent variable.\n",
        "β₀, β₁, β₂, ..., βₙ: are the coefficients of the polynomial terms (y-intercept, slope of the x term, etc.).\n",
        "x, x², x³, ... xⁿ are the powers of the independent variable x.\n",
        "ϵ: is the error term (random error).\n",
        "n: is the degree of the polynomial.\n",
        "\n",
        "\n",
        "27)Can polynomial regression be applied to multiple variables?\n",
        "- Yes, polynomial regression can be applied to multiple variables. It's essentially an extension of multiple linear regression that allows for non-linear relationships between the independent variables and the dependent variable. By including polynomial terms (e.g., squares, cubes) of the independent variables, the model can capture more complex relationships than a simple linear model.\n",
        "\n",
        "\n",
        "28)What are the limitations of polynomial regression?\n",
        "- Polynomial regression, while powerful for modeling complex relationships, has limitations. One main issue is the risk of overfitting, especially with high-degree polynomials, where the model learns the training data too well and performs poorly on unseen data. Another limitation is the computational complexity and the challenge of selecting the optimal polynomial degree, which can impact model performance. Furthermore, polynomial regression can be more sensitive to outliers than linear regression.\n",
        "\n",
        "29)What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Several methods can be used to evaluate model fit when selecting the degree of a polynomial. These include visual inspection of plots, using information criteria like AIC or BIC, and comparing model performance on a validation set.\n",
        "\n",
        "30)Why is visualization important in polynomial regression?\n",
        "- Visualization is crucial in polynomial regression for understanding and interpreting the model's fit to the data, especially when dealing with non-linear relationships. It allows you to visually assess whether the model adequately captures the underlying patterns and avoids overfitting.\n",
        "\n",
        "\n",
        "31)How is polynomial regression implemented in Python?\n"
      ],
      "metadata": {
        "id": "Yl5asKW6o7us"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewQN1p9bo4d5"
      },
      "outputs": [],
      "source": []
    }
  ]
}